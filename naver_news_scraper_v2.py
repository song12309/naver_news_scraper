import requests
from bs4 import BeautifulSoup
import csv
import json
from datetime import datetime
import time
import re
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import getpass
import os

class NaverNewsScraper:
    def __init__(self, history_file='news_history.json'):
        self.base_url = "https://search.naver.com/search.naver"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.history_file = history_file
        self.url_history = self.load_history()

    def search_news(self, keyword, max_results=5):
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            max_results: ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)

        Returns:
            ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        articles = []

        params = {
            'where': 'news',
            'query': keyword,
            'sort': '1',  # ìµœì‹ ìˆœ ì •ë ¬
            'start': 1
        }

        try:
            response = requests.get(self.base_url, params=params, headers=self.headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ìµœì‹  ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡° íŒŒì‹±
            news_items = soup.select('.api_subject_bx')

            count = 0
            for item in news_items:
                if count >= max_results:
                    break

                try:
                    # ì œëª© ì°¾ê¸° (ìƒˆë¡œìš´ êµ¬ì¡°)
                    title_elem = item.select_one('.sds-comps-text-type-headline1')
                    if not title_elem:
                        continue

                    title = title_elem.get_text().strip()

                    # ë§í¬ ì°¾ê¸°
                    link_elem = item.select_one('a[data-heatmap-target=".tit"]')
                    link = link_elem.get('href', '') if link_elem else ''

                    # ì–¸ë¡ ì‚¬ ì°¾ê¸°
                    press_elem = item.select_one('.sds-comps-profile-info-title-text')
                    press = ''
                    if press_elem:
                        press_text = press_elem.get_text().strip()
                        # ì¤‘ì²©ëœ í…ìŠ¤íŠ¸ì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„ë§Œ ì¶”ì¶œ
                        press = re.sub(r'\s+', ' ', press_text).strip()

                    # ë°œí–‰ì¼ ì°¾ê¸°
                    date_elem = item.select_one('.sds-comps-profile-info-subtext')
                    date = ''
                    if date_elem:
                        date = date_elem.get_text().strip()

                    if title and link:
                        article = {
                            'keyword': keyword,
                            'title': title,
                            'link': link,
                            'press': press,
                            'date': date
                        }
                        articles.append(article)
                        count += 1

                except Exception as e:
                    continue

            print(f"'{keyword}' ê²€ìƒ‰ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

        except Exception as e:
            print(f"'{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        return articles

    def load_history(self):
        """
        URL íˆìŠ¤í† ë¦¬ë¥¼ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

        Returns:
            set: ì´ì „ì— ìˆ˜ì§‘í•œ URLë“¤ì˜ ì§‘í•©
        """
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    urls = set(data.get('urls', []))
                    print(f"íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(urls)}ê°œì˜ URL ê¸°ë¡")
                    return urls
            except Exception as e:
                print(f"íˆìŠ¤í† ë¦¬ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return set()
        else:
            print("íˆìŠ¤í† ë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return set()

    def save_history(self):
        """
        í˜„ì¬ URL íˆìŠ¤í† ë¦¬ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        try:
            data = {
                'urls': list(self.url_history),
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {len(self.url_history)}ê°œì˜ URL")
        except Exception as e:
            print(f"íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def is_duplicate(self, url):
        """
        URLì´ ì´ë¯¸ ìˆ˜ì§‘ëœ ì ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

        Args:
            url: í™•ì¸í•  URL

        Returns:
            bool: ì¤‘ë³µì´ë©´ True, ì•„ë‹ˆë©´ False
        """
        return url in self.url_history

    def add_to_history(self, url):
        """
        URLì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            url: ì¶”ê°€í•  URL
        """
        self.url_history.add(url)

    def scrape_multiple_keywords(self, keywords, max_results=5):
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œì— ëŒ€í•´ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ì¤‘ë³µëœ URLì€ ì œì™¸í•©ë‹ˆë‹¤.

        Args:
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_results: ê° í‚¤ì›Œë“œë‹¹ ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜

        Returns:
            ëª¨ë“  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°ëœ)
        """
        all_articles = []
        duplicate_count = 0

        for keyword in keywords:
            articles = self.search_news(keyword, max_results)

            # ì¤‘ë³µ ì²´í¬ ë° í•„í„°ë§
            for article in articles:
                url = article['link']
                if self.is_duplicate(url):
                    duplicate_count += 1
                    print(f"  [ì¤‘ë³µ ì œì™¸] {article['title'][:30]}...")
                else:
                    all_articles.append(article)
                    self.add_to_history(url)

            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´

        if duplicate_count > 0:
            print(f"\nì´ {duplicate_count}ê°œì˜ ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")

        return all_articles

    def save_to_csv(self, articles, filename='naver_news_results.csv'):
        """
        ìˆ˜ì§‘í•œ ë‰´ìŠ¤ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if not articles:
            print("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['keyword', 'title', 'link', 'press', 'date']
            writer = csv.DictWriter(f, fieldnames=fieldnames)

            writer.writeheader()
            for article in articles:
                writer.writerow(article)

        print(f"CSV íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: {filename}")

    def save_to_json(self, articles, filename='naver_news_results.json'):
        """
        ìˆ˜ì§‘í•œ ë‰´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if not articles:
            print("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

        print(f"JSON íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: {filename}")

    def send_email(self, articles, gmail_user, gmail_password, recipient_email):
        """
        ìˆ˜ì§‘í•œ ë‰´ìŠ¤ë¥¼ HTML í˜•ì‹ì˜ ì´ë©”ì¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

        Args:
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            gmail_user: Gmail ê³„ì • (ë°œì‹ ì)
            gmail_password: Gmail ì•± ë¹„ë°€ë²ˆí˜¸
            recipient_email: ìˆ˜ì‹ ì ì´ë©”ì¼
        """
        if not articles:
            print("ì „ì†¡í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # ì´ë©”ì¼ ì œëª©
        subject = f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ - {datetime.now().strftime('%Y-%m-%d %H:%M')}"

        # HTML ì´ë©”ì¼ ë³¸ë¬¸ ìƒì„±
        html_body = self._generate_html_email(articles)

        # ì´ë©”ì¼ ë©”ì‹œì§€ ìƒì„±
        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = gmail_user
        msg['To'] = recipient_email

        # HTML íŒŒíŠ¸ ì¶”ê°€
        html_part = MIMEText(html_body, 'html', 'utf-8')
        msg.attach(html_part)

        try:
            # Gmail SMTP ì„œë²„ ì—°ê²°
            print("ì´ë©”ì¼ ì „ì†¡ ì¤‘...")
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(gmail_user, gmail_password)

            # ì´ë©”ì¼ ì „ì†¡
            server.send_message(msg)
            server.quit()

            print(f"âœ“ ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ: {recipient_email}")
            return True

        except smtplib.SMTPAuthenticationError:
            print("âœ— ì´ë©”ì¼ ì¸ì¦ ì‹¤íŒ¨. Gmail ê³„ì • ë˜ëŠ” ì•± ë¹„ë°€ë²ˆí˜¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            print("  ì•± ë¹„ë°€ë²ˆí˜¸ ìƒì„±: https://myaccount.google.com/apppasswords")
            return False
        except Exception as e:
            print(f"âœ— ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False

    def _generate_html_email(self, articles):
        """
        HTML í˜•ì‹ì˜ ì´ë©”ì¼ ë³¸ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # í‚¤ì›Œë“œë³„ë¡œ ê¸°ì‚¬ ê·¸ë£¹í™”
        grouped = {}
        for article in articles:
            keyword = article['keyword']
            if keyword not in grouped:
                grouped[keyword] = []
            grouped[keyword].append(article)

        # HTML í…œí”Œë¦¿
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <style>
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
                    line-height: 1.6;
                    color: #333;
                    max-width: 800px;
                    margin: 0 auto;
                    padding: 20px;
                    background-color: #f5f5f5;
                }}
                .container {{
                    background-color: #ffffff;
                    border-radius: 8px;
                    padding: 30px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                h1 {{
                    color: #03C75A;
                    border-bottom: 3px solid #03C75A;
                    padding-bottom: 10px;
                    margin-bottom: 30px;
                }}
                h2 {{
                    color: #1a73e8;
                    margin-top: 30px;
                    margin-bottom: 15px;
                    font-size: 1.3em;
                }}
                .article {{
                    background-color: #f8f9fa;
                    border-left: 4px solid #1a73e8;
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 4px;
                    transition: all 0.3s ease;
                }}
                .article:hover {{
                    background-color: #e8f0fe;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                }}
                .article-title {{
                    font-size: 1.1em;
                    font-weight: bold;
                    margin-bottom: 8px;
                }}
                .article-title a {{
                    color: #1a73e8;
                    text-decoration: none;
                }}
                .article-title a:hover {{
                    text-decoration: underline;
                }}
                .article-meta {{
                    color: #666;
                    font-size: 0.9em;
                    margin-top: 8px;
                }}
                .press {{
                    display: inline-block;
                    background-color: #e8f0fe;
                    padding: 2px 8px;
                    border-radius: 3px;
                    margin-right: 10px;
                    font-weight: 500;
                }}
                .date {{
                    color: #999;
                }}
                .summary {{
                    background-color: #e8f5e9;
                    padding: 15px;
                    border-radius: 4px;
                    margin-bottom: 30px;
                }}
                .footer {{
                    margin-top: 40px;
                    padding-top: 20px;
                    border-top: 1px solid #ddd;
                    text-align: center;
                    color: #999;
                    font-size: 0.9em;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ê²°ê³¼</h1>

                <div class="summary">
                    <strong>ìˆ˜ì§‘ ì‹œê°„:</strong> {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}<br>
                    <strong>ì´ ê¸°ì‚¬ ìˆ˜:</strong> {len(articles)}ê°œ<br>
                    <strong>í‚¤ì›Œë“œ:</strong> {', '.join(grouped.keys())}
                </div>
        """

        # í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ì¶”ê°€
        for keyword, keyword_articles in grouped.items():
            html += f"""
                <h2>ğŸ” {keyword} ({len(keyword_articles)}ê°œ)</h2>
            """

            for article in keyword_articles:
                html += f"""
                <div class="article">
                    <div class="article-title">
                        <a href="{article['link']}" target="_blank">{article['title']}</a>
                    </div>
                    <div class="article-meta">
                        <span class="press">{article['press']}</span>
                        <span class="date">{article['date']}</span>
                    </div>
                </div>
                """

        html += """
                <div class="footer">
                    ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ by Python<br>
                    ì´ ì´ë©”ì¼ì€ ìë™ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
                </div>
            </div>
        </body>
        </html>
        """

        return html


def main():
    print("=" * 50)
    print("ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    print("=" * 50)

    # ì´ë©”ì¼ ì „ì†¡ ì˜µì…˜
    print("\nì´ë©”ì¼ ì „ì†¡ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
    send_email_option = input().strip().lower()

    GMAIL_USER = None
    GMAIL_PASSWORD = None
    RECIPIENT_EMAIL = None

    if send_email_option == 'y':
        print("\nì´ë©”ì¼ ì „ì†¡ ì„¤ì •")
        print("-" * 50)
        GMAIL_USER = input("ë°œì‹ ì Gmail ê³„ì • ì…ë ¥: ").strip()
        GMAIL_PASSWORD = getpass.getpass("Gmail ì•± ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ (ì…ë ¥ ë‚´ìš©ì´ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤): ").strip()
        RECIPIENT_EMAIL = input("ìˆ˜ì‹ ì ì´ë©”ì¼ ì…ë ¥: ").strip()

        if not GMAIL_USER or not GMAIL_PASSWORD or not RECIPIENT_EMAIL:
            print("\nâœ— ì´ë©”ì¼ ì •ë³´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì´ë©”ì¼ ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            send_email_option = 'n'

    # ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    keywords = [
        'ì•¼ë†€ì',
        'ì—¬ê¸°ì–´ë•Œ',
        'ì•„ê³ ë‹¤',
        'ìµìŠ¤í”¼ë””ì•„',
        'ì—ì–´ë¹„ì•¤ë¹„',
        'í˜¸í…”ìŠ¤ë‹·ì»´',
        'íŠ¸ë¦½ë‹·ì»´',
        'ìŠ¤í…Œì´í´ë¦¬ì˜¤',
        'ë§ˆì´ë¦¬ì–¼íŠ¸ë¦½'
    ]

    print("\n" + "=" * 50)
    print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}")
    print(f"ê° í‚¤ì›Œë“œë‹¹ ìˆ˜ì§‘ ê°œìˆ˜: 5ê°œ")
    print("=" * 50)

    scraper = NaverNewsScraper()

    # ë‰´ìŠ¤ ìˆ˜ì§‘
    articles = scraper.scrape_multiple_keywords(keywords, max_results=5)

    print("\n" + "=" * 50)
    print(f"ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    print("=" * 50)

    # ê²°ê³¼ ì¶œë ¥
    for article in articles:
        print(f"\n[{article['keyword']}] {article['title']}")
        print(f"  ì–¸ë¡ ì‚¬: {article['press']}")
        print(f"  ë°œí–‰ì¼: {article['date']}")
        print(f"  ë§í¬: {article['link']}")

    # íŒŒì¼ë¡œ ì €ì¥
    scraper.save_to_csv(articles)
    scraper.save_to_json(articles)

    # ì´ë©”ì¼ ì „ì†¡
    if send_email_option == 'y' and GMAIL_USER and GMAIL_PASSWORD and RECIPIENT_EMAIL:
        print("\n" + "=" * 50)
        scraper.send_email(articles, GMAIL_USER, GMAIL_PASSWORD, RECIPIENT_EMAIL)
        print("=" * 50)

    # íˆìŠ¤í† ë¦¬ ì €ì¥
    print("\n" + "=" * 50)
    scraper.save_history()
    print("=" * 50)

    print("\nì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    main()
